{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Requirements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries, functions and classes that are used along the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat, savemat\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "from scipy.stats import mode\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, r'C:\\Users\\fscielzo\\Documents\\Packages\\PyMachineLearning_Package_Private')\n",
    "from PyMachineLearning.evaluation import SimpleEvaluation\n",
    "from PyMachineLearning.preprocessing import scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to extract signals features based on block-segmentation, statistics and concatenation.\n",
    "\n",
    "  This function uses raw signals data as input and returns a predictors matrix as output, suitable to be used with classic Machine Learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_features_HAR(X_HAR, blocks_size, statistics, verbose=False):\n",
    "\n",
    "    # X_HAR: list with the signals data matrix of each individual\n",
    "\n",
    "    n_individuals = len(X_HAR)\n",
    "    X_individual, signals_block = {}, {}\n",
    "\n",
    "    for i in range(n_individuals):\n",
    "        \n",
    "        # signals matrix of i-th individual\n",
    "        signals = X_HAR[i].T\n",
    "        # num. samples for i-th individual\n",
    "        n_samples = len(signals)\n",
    "        # Time during which human activity was recorded\n",
    "        if verbose:\n",
    "            print(f'Time during which the activity of individual {i+1} of X_HAR was recorded:', np.round(n_samples/(blocks_size*60),3), 'mins.')\n",
    "        # num. blocks to segmented data of i-th individual. \n",
    "        n_blocks = int(np.floor(n_samples/blocks_size))\n",
    "        # number of samples not assigned to any block\n",
    "        n_remaining_samples = n_samples - n_blocks*blocks_size\n",
    "        \n",
    "        # Segmenting the i-th individual data in blocks\n",
    "        signals_block[i] = []\n",
    "        for b in range(0, n_blocks):\n",
    "            signals_block[i].append(signals[b*blocks_size:((b+1)*blocks_size),:])\n",
    "        # Adding the remaining samples to the last block\n",
    "        signals_block[i][-1] = np.row_stack([signals_block[i][-1], signals[-n_remaining_samples:,:]])\n",
    "\n",
    "        # Computing statistics for each signal-data block and most frequent label per block for the i-th individual\n",
    "        X_individual[i] = []\n",
    "        for b in range(0, n_blocks):\n",
    "\n",
    "            if statistics == 'mean':\n",
    "                # mean of the signals features for i-th individual\n",
    "                X_individual[i].append(np.mean(signals_block[i][b], axis=0))\n",
    "        \n",
    "            elif statistics == 'mean-std':\n",
    "                # mean-std of the signals features for i-th individual\n",
    "                X_features_mean = np.mean(signals_block[i][b], axis=0)\n",
    "                X_features_std = np.std(signals_block[i][b], axis=0)\n",
    "                X_individual[i].append(np.hstack([X_features_mean, X_features_std]))\n",
    "          \n",
    "            elif statistics == 'mean-median-std':\n",
    "                # mean-std of the signals features for i-th individual\n",
    "                X_features_mean = np.mean(signals_block[i][b], axis=0)\n",
    "                X_features_median = np.median(signals_block[i][b], axis=0)\n",
    "                X_features_std = np.std(signals_block[i][b], axis=0)\n",
    "                X_individual[i].append(np.hstack([X_features_mean, X_features_median, X_features_std]))\n",
    "      \n",
    "            elif statistics == 'mean-Q25-median-Q75-std':\n",
    "                # mean-std of the signals features for i-th individual\n",
    "                X_features_mean = np.mean(signals_block[i][b], axis=0)\n",
    "                X_features_Q25 = np.quantile(signals_block[i][b], q=0.25, axis=0)\n",
    "                X_features_median = np.median(signals_block[i][b], axis=0)\n",
    "                X_features_Q75 = np.quantile(signals_block[i][b], q=0.75, axis=0)\n",
    "                X_features_std = np.std(signals_block[i][b], axis=0)\n",
    "                X_individual[i].append(np.hstack([X_features_mean, X_features_Q25, X_features_median, X_features_Q75, X_features_std]))\n",
    "        \n",
    "        # Building an array with the signal statistics for each data-block, this is the features matrix for the i-th person. \n",
    "        # Rows represent one second and are the observations.\n",
    "        # Columns represent a signal and are the features (predictors).\n",
    "        X_individual[i] = np.array(X_individual[i])\n",
    "\n",
    "    X = np.row_stack([X_individual[i] for i in X_individual.keys()])\n",
    "\n",
    "    return X, X_individual, signals_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to process the given response data and making it compatible with the predictors matrix returned by the above function. This function follows a very similar approach to `get_X_features_HAR`, in a nutsell, it is based  on block-segmentation, the mode (as statistic) and concatenation.\n",
    "\n",
    "  This function activity classes as input data and returns a response array with those classes processed correctly as output. This output could be used along with the predictors matrix given by `get_X_features_HAR` as the inputs of classic Machine Learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_features_HAR(Y_HAR, blocks_size, verbose=False):\n",
    "\n",
    "    # Y_HAR: list with the response vector of each person\n",
    "\n",
    "    n_individuals = len(Y_HAR)\n",
    "    Y_individual, classes_block = {}, {}\n",
    "\n",
    "    for i in range(n_individuals):\n",
    "        \n",
    "        # response vector of i-th person\n",
    "        classes = Y_HAR[i] \n",
    "        # num. samples for i-th person\n",
    "        n_samples = len(classes)\n",
    "        # Time during which human activity was recorded\n",
    "        if verbose:\n",
    "            print(f'Time during which the activity of person {i+1} of Y_HAR was recorded:', np.round(n_samples/(blocks_size*60),3), 'mins.')\n",
    "        # num. blocks to segmented response vector of i-th person. \n",
    "        n_blocks = int(np.floor(n_samples/blocks_size))\n",
    "        # number of samples not assigned to any block\n",
    "        n_remaining_samples = n_samples - n_blocks*blocks_size\n",
    "        \n",
    "        # Segmenting the i-th person data in blocks\n",
    "        # Each block represent {blocks_size/sampling_freq} second/s, since the sampling frequency represents 1 second.\n",
    "        classes_block[i] = []\n",
    "        for b in range(0, n_blocks):\n",
    "            classes_block[i].append(classes[b*blocks_size:((b+1)*blocks_size)])\n",
    "        # Adding the remaining samples to the last block\n",
    "        classes_block[i][-1] = np.hstack([classes_block[i][-1], classes[-n_remaining_samples:]])\n",
    "\n",
    "        # Computing statistics for each signal-data block and most frequent label per block for the i-th person\n",
    "        Y_individual[i] = []\n",
    "        for b in range(0, n_blocks):\n",
    "            Y_individual[i].append(mode(classes_block[i][b])[0])\n",
    "        \n",
    "        # Building an array with the classes for each data-block, this is the response vector for the i-th person.\n",
    "        Y_individual[i] = np.array(Y_individual[i])\n",
    "\n",
    "    Y = np.concatenate([Y_individual[i] for i in Y_individual.keys()])\n",
    "\n",
    "    return Y, Y_individual, classes_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A class that implements `get_X_features_HAR` following the `Sklearn` transformers rules. `FeaturesExtractionHAR`is indeed an `Sklearn` transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesExtractionHAR(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, blocks_size, statistics, verbose=False):\n",
    "       \n",
    "        self.blocks_size = blocks_size\n",
    "        self.statistics = statistics\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "\n",
    "        X, self.X_individual, self.signals_block = get_X_features_HAR(X_HAR=X, blocks_size=self.blocks_size, \n",
    "                                                                      statistics=self.statistics, verbose=self.verbose)\n",
    "\n",
    "        return X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
